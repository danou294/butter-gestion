#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'import end-to-end pour la collection 'restaurants' dans Firebase Firestore
Adapt√© pour Django
"""

import os
import re
import ast
import csv
import json
import sys
import hashlib
import pathlib
import traceback
import datetime
import time
import urllib.parse
import urllib.request
from collections import defaultdict, Counter
from typing import Dict, Any, List, Optional, Tuple

import pandas as pd
import firebase_admin
from firebase_admin import credentials, firestore

try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except Exception:
    ZoneInfo = None

# Import de la config Django
from config import FIRESTORE_COLLECTION, INPUT_DIR, EXPORTS_DIR, BACKUP_DIR

# -------------------- Config --------------------
COLLECTION_SOURCE = FIRESTORE_COLLECTION
COLLECTION_IMPORT_LOGS = "import logs"
BATCH_SIZE = 400
DEDUPE_IDS = False

# -------------------- Utilities --------------------
def ensure_dir(p: str):
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)

def now_paris_str() -> str:
    dt = datetime.datetime.now(datetime.timezone.utc)
    if ZoneInfo is not None:
        local = dt.astimezone(ZoneInfo("Europe/Paris"))
        return local.strftime("%d %b %Y √† %H:%M:%S %Z%z")
    else:
        return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()

def log(msg: str, log_file: str):
    print(msg)
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(msg + "\n")

def init_firestore(log_file: str, request=None):
    """
    Initialise Firestore avec le bon environnement
    
    Args:
        log_file: Chemin du fichier de log
        request: Objet request Django (optionnel) pour d√©terminer l'environnement
    """
    # Utiliser firebase_utils pour obtenir le bon chemin selon l'environnement
    try:
        from scripts_manager.firebase_utils import get_service_account_path
        sa = get_service_account_path(request)
    except ImportError:
        # Fallback si firebase_utils n'est pas disponible
        from config import SERVICE_ACCOUNT_PATH_DEV, SERVICE_ACCOUNT_PATH_PROD
        # Ne pas red√©clarer 'os' ici, utiliser le module import√© au d√©but du fichier
        env = os.getenv('FIREBASE_ENV', 'prod').lower()
        if env == 'dev':
            sa = SERVICE_ACCOUNT_PATH_DEV
        else:
            sa = SERVICE_ACCOUNT_PATH_PROD
    
    if not os.path.exists(sa):
        log(f"‚ùå Service account introuvable: {sa}", log_file)
        raise FileNotFoundError(f"Service account introuvable: {sa}")
    
    log(f"üîë Utilisation du service account: {sa}", log_file)
    cred = credentials.Certificate(sa)
    if not firebase_admin._apps:
        firebase_admin.initialize_app(cred)
    return firestore.client()

def clean_text(s):
    """Nettoie le texte d'une cellule Excel"""
    if s is None or (isinstance(s, float) and pd.isna(s)):
        return ""
    s = str(s).strip()
    return (s.replace("\u202f", " ")
             .replace("\u2009", " ")
             .replace(" ", " ")
             .replace("\xa0", " ")
             .replace("‚Äì", "-")
             .replace("‚Äî", "-")
             .strip())

def geocode_address(address: str, log_file: str = None, max_retries: int = 3, restaurant_name: str = None) -> Optional[Tuple[float, float]]:
    """G√©ocode une adresse en utilisant Nominatim (OpenStreetMap) avec syst√®me de retry."""
    if not address or not address.strip():
        if log_file:
            log(f"‚ö†Ô∏è  G√©ocodage: adresse vide ou None", log_file)
        return None
    
    address_clean = clean_text(address)
    if not address_clean:
        if log_file:
            log(f"‚ö†Ô∏è  G√©ocodage: adresse nettoy√©e vide", log_file)
        return None
    
    # Ajouter Paris si n√©cessaire
    original_address = address_clean
    if "paris" not in address_clean.lower():
        address_clean = f"{address_clean}, Paris, France"
        if log_file:
            log(f"üåç [{restaurant_name or 'Restaurant'}] Adresse enrichie: '{original_address}' ‚Üí '{address_clean}'", log_file)
    
    encoded_address = urllib.parse.quote(address_clean)
    url = f"https://nominatim.openstreetmap.org/search?q={encoded_address}&format=json&limit=1&addressdetails=1"
    
    if log_file:
        log(f"üåê [{restaurant_name or 'Restaurant'}] Appel API Nominatim pour: '{address_clean}'", log_file)
    
    for attempt in range(1, max_retries + 1):
        try:
            if attempt > 1:
                wait_time = min(2 ** (attempt - 1), 10)
                if log_file:
                    log(f"üîÑ [{restaurant_name or 'Restaurant'}] G√©ocodage: tentative {attempt}/{max_retries} pour '{address_clean[:60]}...' (attente {wait_time}s)", log_file)
                time.sleep(wait_time)
            else:
                if log_file:
                    log(f"üîÑ [{restaurant_name or 'Restaurant'}] G√©ocodage: tentative {attempt}/{max_retries} pour '{address_clean[:60]}...'", log_file)
            
            req = urllib.request.Request(url)
            req.add_header('User-Agent', 'RestaurantImportScript/1.0')
            req.add_header('Accept', 'application/json')
            
            if log_file:
                log(f"üì° [{restaurant_name or 'Restaurant'}] Envoi requ√™te HTTP √† Nominatim...", log_file)
            
            with urllib.request.urlopen(req, timeout=30) as response:
                response_data = response.read()
                if log_file:
                    log(f"üì• [{restaurant_name or 'Restaurant'}] R√©ponse re√ßue ({len(response_data)} bytes)", log_file)
                
                data = json.loads(response_data.decode())
                
                if data and len(data) > 0:
                    result = data[0]
                    lat = float(result.get('lat', 0))
                    lon = float(result.get('lon', 0))
                    
                    if log_file:
                        log(f"üìç [{restaurant_name or 'Restaurant'}] Coordonn√©es brutes de l'API: lat={lat}, lon={lon}", log_file)
                    
                    if lat != 0 and lon != 0:
                        if log_file:
                            log(f"‚úÖ [{restaurant_name or 'Restaurant'}] G√©ocodage r√©ussi: '{address_clean[:60]}...' ‚Üí ({lat:.6f}, {lon:.6f})", log_file)
                        return (lat, lon)
                    else:
                        if log_file:
                            log(f"‚ö†Ô∏è  [{restaurant_name or 'Restaurant'}] Coordonn√©es invalides (0,0) re√ßues de l'API", log_file)
                else:
                    if log_file:
                        log(f"‚ö†Ô∏è  [{restaurant_name or 'Restaurant'}] Aucun r√©sultat dans la r√©ponse de l'API", log_file)
            
            if attempt == max_retries and log_file:
                log(f"‚ùå [{restaurant_name or 'Restaurant'}] G√©ocodage √©chou√© apr√®s {max_retries} tentatives: '{address_clean[:60]}...' (aucun r√©sultat valide)", log_file)
            
        except urllib.error.URLError as e:
            if log_file:
                log(f"‚ùå [{restaurant_name or 'Restaurant'}] Erreur r√©seau lors du g√©ocodage (tentative {attempt}/{max_retries}): {type(e).__name__} - {str(e)[:100]}", log_file)
            if attempt == max_retries:
                if log_file:
                    log(f"‚ùå [{restaurant_name or 'Restaurant'}] G√©ocodage d√©finitivement √©chou√©: erreur r√©seau", log_file)
            continue
        except Exception as e:
            if log_file:
                log(f"‚ùå [{restaurant_name or 'Restaurant'}] Erreur inattendue lors du g√©ocodage (tentative {attempt}/{max_retries}): {type(e).__name__} - {str(e)[:100]}", log_file)
            if attempt == max_retries:
                if log_file:
                    log(f"‚ùå [{restaurant_name or 'Restaurant'}] G√©ocodage d√©finitivement √©chou√©: {type(e).__name__}", log_file)
            continue
    
    return None

# -------------------- Backup --------------------
def export_collection(db, collection_name: str, out_dir: str, log_file: str) -> Dict[str, Any]:
    ensure_dir(out_dir)
    json_path = os.path.join(out_dir, f"{collection_name}.json")
    ndjson_path = os.path.join(out_dir, f"{collection_name}.ndjson")
    csv_path = os.path.join(out_dir, f"{collection_name}.csv")
    meta_path = os.path.join(out_dir, "backup_meta.json")

    docs = list(db.collection(collection_name).stream())
    count = len(docs)
    data_array = []

    with open(ndjson_path, "w", encoding="utf-8") as nf:
        for d in docs:
            obj = d.to_dict() or {}
            row = {"id": d.id, **obj}
            data_array.append(row)
            nf.write(json.dumps(row, ensure_ascii=False) + "\n")
    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(data_array, jf, ensure_ascii=False, indent=2)

    with open(csv_path, "w", encoding="utf-8", newline="") as cf:
        writer = csv.DictWriter(cf, fieldnames=["id", "name", "address", "arrondissement", "tag"])
        writer.writeheader()
        for r in data_array:
            writer.writerow({
                "id": r.get("id", ""),
                "name": r.get("name", ""),
                "address": r.get("address", ""),
                "arrondissement": r.get("arrondissement", ""),
                "tag": r.get("tag", ""),
            })

    meta = {
        "collection": collection_name,
        "count": count,
        "json": os.path.basename(json_path),
        "ndjson": os.path.basename(ndjson_path),
        "csv": os.path.basename(csv_path),
        "timestamp": now_paris_str(),
        "sha256_json": sha256_file(json_path),
        "sha256_ndjson": sha256_file(ndjson_path),
    }
    with open(meta_path, "w", encoding="utf-8") as mf:
        json.dump(meta, mf, ensure_ascii=False, indent=2)

    log(f"‚úÖ Backup OK ({count} docs) ‚Üí {out_dir}", log_file)
    return {"count": count, "dir": out_dir, **meta}

# -------------------- Excel ‚Üí records --------------------
def to_list(v):
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return []
    if isinstance(v, list):
        return [str(x).strip() for x in v if str(x).strip()]
    s = clean_text(v)
    if not s:
        return []
    if s.startswith("[") and s.endswith("]"):
        try:
            items = ast.literal_eval(s)
            if isinstance(items, (list, tuple)):
                return [str(x).strip() for x in items if str(x).strip()]
        except Exception:
            pass
    parts = re.split(r"[,/;|¬∑]", s)
    return [p.strip() for p in parts if p.strip()]

def string_to_tag_list(v):
    """Convertit une string de tags s√©par√©s par des virgules en liste de tags nettoy√©s"""
    if not v or (isinstance(v, float) and pd.isna(v)):
        return []
    s = clean_text(v)
    if not s or s.lower() in ["non", "nan", ""]:
        return []
    tags = [tag.strip() for tag in s.split(",") if tag.strip()]
    return [tag for tag in tags if tag.lower() not in ["non", "nan", ""]]

DAYS_EN = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
DAYS_FR = ["Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi", "Dimanche"]
DAY_EN_TO_FR = dict(zip(DAYS_EN, DAYS_FR))
DAY_PATTERN = r"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Lundi|Mardi|Mercredi|Jeudi|Vendredi|Samedi|Dimanche):\s*"
RE_12H = re.compile(r"(\d{1,2}:\d{2})\s*([APMapm]{2})?\s*-\s*(\d{1,2}:\d{2})\s*([APMapm]{2})")
RE_24H = re.compile(r"(\d{1,2}:\d{2})\s*-\s*(\d{1,2}:\d{2})")

def to_24h_safe(hour_str, suffix=""):
    s = hour_str.strip()
    if re.fullmatch(r"\d{1,2}:\d{2}", s) and not suffix:
        return s
    try:
        return datetime.datetime.strptime(f"{s} {suffix}".strip(), "%I:%M %p").strftime("%H:%M")
    except Exception:
        return s

def parse_bracketed_list_if_any(hours_str):
    t = clean_text(hours_str)
    if t.startswith("[") and t.endswith("]"):
        try:
            items = ast.literal_eval(t)
            if isinstance(items, (list, tuple)):
                return "\n".join(str(x).strip() for x in items if str(x).strip())
        except Exception:
            return t
    return t

def extract_slot_range(slot):
    slot = clean_text(slot)
    m12 = RE_12H.search(slot)
    if m12:
        h1, s1, h2, s2 = m12.groups()
        s1 = s1 or s2
        return f"{to_24h_safe(h1, s1)} - {to_24h_safe(h2, s2)}"
    m24 = RE_24H.search(slot)
    if m24:
        h1, h2 = m24.groups()
        return f"{to_24h_safe(h1)} - {to_24h_safe(h2)}"
    return None

def parse_day_slots(text):
    text = clean_text(text)
    if not text or re.search(r"\b(closed|ferm[√©e]?)\b", text, flags=re.I):
        return {"closed": True}
    slots = [s.strip() for s in re.split(r"[;,]", text) if s.strip()]
    result = {}
    idx = 1
    for s in slots:
        rng = extract_slot_range(s)
        if rng:
            result[f"service_{idx}"] = rng
            idx += 1
    return result if result else {"closed": True}

def split_hours_by_day(hours_str):
    text = parse_bracketed_list_if_any(hours_str)
    text = re.sub(DAY_PATTERN, r"\n\1: ", clean_text(text))
    parts = re.split(DAY_PATTERN, text)
    chunks = {}
    for i in range(1, len(parts), 2):
        day = parts[i]
        value = parts[i + 1].strip()
        fr_day = DAY_EN_TO_FR.get(day, day)
        chunks[fr_day] = value
    return chunks

def process_hours(hours_str):
    if not clean_text(hours_str):
        return {}
    day_blocks = split_hours_by_day(hours_str)
    structured = {fr_day: parse_day_slots(value) for fr_day, value in day_blocks.items()}
    for d in DAYS_FR:
        structured.setdefault(d, {"closed": True})
    return structured

def arrondissement_to_code_postal(v):
    """Convertit la valeur d'arrondissement en code postal complet"""
    s = clean_text(v)
    if not s:
        return ""
    if re.match(r"^\d{5}$", s):
        return s
    try:
        n = int(s)
        if 1 <= n <= 20:
            return f"750{n:02d}"
    except Exception:
        pass
    return s

def normalize_id_from_tag(tag):
    rid = re.sub(r"[^a-zA-Z0-9_-]+", "-", tag).strip("-").upper()
    return rid

TAG_GROUPS = {
    "type_tags": ['Restaurant', 'Restaurant haut de gamme', 'Restaurant gastronomique', 'Restaurant √©toil√©',
                         'Brasserie', 'Cave √† manger', 'Fast',
                         'Boulangerie/p√¢tisserie', 'Concept brunch', 'Concept go√ªter',
                         'Coffee shop/salon de th√©', 'Bar'],
    "moment": ['Petit-d√©jeuner', 'Brunch', 'D√©jeuner', 'Go√ªter', 'Ap√©ro', 'D√Æner', 'Sans r√©servation'],
    "location_type": ['Rue', 'Bar', 'Brasserie', 'Cave √† manger', 'Restaurant', 'Fast', 'Hotel', '√âtoil√©', 'Coffee Shop', 'Salle privatisable', 'Terrasse'],
    "location_name": [],
    "lieu_tags": ['Rue', 'Bar', 'Brasserie', 'Cave √† manger', 'Restaurant', 'Fast', 'Hotel', '√âtoil√©', 'Coffee Shop', 'Salle privatisable', 'Terrasse'],
    "ambiance": ['Entre amis', 'En famille', 'Date', 'Festif'],
    "price_range": ['‚Ç¨', '‚Ç¨‚Ç¨', '‚Ç¨‚Ç¨‚Ç¨', '‚Ç¨‚Ç¨‚Ç¨‚Ç¨'],
    "cuisine": ['Africain', 'Am√©ricain', 'Chinois', 'Cor√©en', 'Colombien', 'Fran√ßais', 'Grec', 'Indien',
                 'Isra√©lien', 'Italien', 'Japonais', 'Libanais', 'Mexicain', 'Oriental', 'P√©ruvien',
                 'Sud-Am√©ricain', 'Tha√Ø', 'Vietnamien'],
    "preferences": ['Casher', '100% v√©g√©tarien', 'Healthy'],
    "terrace": ['Terrasse', 'Terrasse classique', 'Cour', 'Rooftop'],
    "recommended_by": []  # Pas de valeurs pr√©d√©finies, ce sont des tags libres
}

def collect_type_tags_from_columns(row):
    """Collecte des tags de type depuis les colonnes _TAG appropri√©es"""
    try:
        type_tags = []
        for col in ['Sp√©cialit√©_TAG', 'Lieu_TAG', 'Ambiance_TAG']:
            if col in row.index:
                val = row[col]
                if pd.notna(val) and str(val).strip().lower() not in ["non", "", "nan"]:
                    type_tags.append(str(val).strip())
        return ", ".join(type_tags)
    except Exception as e:
        return ""

def collect_location_name_from_columns(row):
    """Collecte du nom du lieu depuis les colonnes _AFFICHAGE appropri√©es"""
    try:
        location_names = []
        for col in ['Lieu_AFFICHAGE', 'Ambiance_AFFICHAGE']:
            if col in row.index:
                val = row[col]
                if pd.notna(val) and str(val).strip().lower() not in ["non", "", "nan"]:
                    location_names.append(str(val).strip())
        return ", ".join(location_names)
    except Exception as e:
        return ""

def collect_tags_from_excel_columns(row, tag_group_name):
    """Collecte des tags depuis les colonnes Excel _TAG et _AFFICHAGE"""
    try:
        results = []
        column_mapping = {
            "type_tags": ['Sp√©cialit√©_TAG'],
            "moment": ['Moment_TAG'],
            "location_type": ['Lieu_TAG'],
            "lieu_tags": ['Lieu_TAG'],
            "ambiance": ['Ambiance_TAG'],
            "price_range": ['Prix_TAG'],
            "cuisine": ['Sp√©cialit√©_TAG'],
            "preferences": ['Pr√©f√©rences_TAG', 'Restrictions_TAG', 'Pr√©f√©rences'],  # Support ancien et nouveau nom, et format sans _TAG
            "terrace": ['Lieu_TAG', 'Ambiance_TAG'],
            "recommended_by": ['recommand√© par - TAG']
        }
        columns_to_check = column_mapping.get(tag_group_name, [])
        for col in columns_to_check:
            if col in row.index:
                val = row[col]
                if pd.notna(val) and str(val).strip().lower() not in ["non", "", "nan"]:
                    val_str = str(val).strip()
                    tags = [tag.strip() for tag in val_str.split(",") if tag.strip()]
                    for tag in tags:
                        if tag_group_name == "terrace":
                            if "terrasse" in tag.lower() or "rooftop" in tag.lower() or "cour" in tag.lower():
                                if tag not in results:
                                    results.append(tag)
                        elif tag_group_name == "preferences":
                            # Normalisation des pr√©f√©rences
                            tag_lower = tag.lower().strip()
                            if "casher" in tag_lower:
                                clean_tag = "Casher"
                            elif "v√©g√©tarien" in tag_lower or "vegetarien" in tag_lower:
                                clean_tag = "100% v√©g√©tarien"
                            elif "healthy" in tag_lower:
                                clean_tag = "Healthy"
                            else:
                                # Garder la valeur telle quelle si elle correspond aux nouvelles valeurs
                                clean_tag = tag
                            if clean_tag not in results:
                                results.append(clean_tag)
                        elif tag_group_name == "recommended_by":
                            # Tags libres pour "recommand√© par"
                            if tag not in results:
                                results.append(tag)
                        else:
                            if tag not in results:
                                results.append(tag)
        return results
    except Exception as e:
        return []

def collect_affichage_tags(row):
    """Collecte tous les tags d'affichage (sauf sp√©cialit√©) et retourne une liste"""
    affichage_parts = []
    affichage_columns = [
        "Moment_AFFICHAGE",
        "Ambiance_AFFICHAGE", 
        "Lieu_AFFICHAGE",
        "Pr√©f√©rences_AFFICHAGE",
        "Restrictions_AFFICHAGE",  # Support ancien nom pour compatibilit√©
        "recommand√© par - AFFICHAGE"
    ]
    for col in affichage_columns:
        if col in row.index:
            value = str(row[col]).strip()
            if value and value.lower() not in ["", "nan", "non"]:
                items = [item.strip() for item in value.split(",") if item.strip()]
                for item in items:
                    if item not in affichage_parts:
                        affichage_parts.append(item)
    return affichage_parts

def collect_specialite_affichage(row):
    """Collecte uniquement Sp√©cialit√©_AFFICHAGE"""
    if "Sp√©cialit√©_AFFICHAGE" in row.index:
        value = str(row["Sp√©cialit√©_AFFICHAGE"]).strip()
        if value and value.lower() not in ["", "nan", "non"]:
            return value
    return ""

def convert_excel(excel_path: str, sheet_name: str, out_json: str, out_ndjson: str, out_csv: str, log_file: str):
    if not os.path.exists(excel_path):
        raise FileNotFoundError(f"Excel introuvable: {excel_path}")
    
    xls = pd.ExcelFile(excel_path)
    log(f"üìã Feuilles disponibles: {xls.sheet_names}", log_file)
    
    if not xls.sheet_names:
        raise ValueError("Aucune feuille trouv√©e dans le fichier Excel")
    
    if sheet_name not in xls.sheet_names:
        sheet_name = xls.sheet_names[0]
        log(f"‚ö†Ô∏è  Feuille '{sheet_name}' non trouv√©e, utilisation de: {sheet_name}", log_file)
    
    df = xls.parse(sheet_name)
    log(f"üìä Donn√©es charg√©es: {len(df)} lignes, {len(df.columns)} colonnes", log_file)
    
    if len(df) == 0:
        raise ValueError("Le fichier Excel ne contient aucune donn√©e")
    
    # Utiliser la premi√®re ligne comme en-t√™tes
    df.columns = df.iloc[0]
    rows = df.iloc[1:].copy()
    log(f"üìù Donn√©es √† traiter: {len(rows)} lignes", log_file)
    
    # Variables pour le g√©ocodage (utiliser des listes pour permettre la modification dans la fonction interne)
    geocoding_counter = [0]
    geocoding_stats = {"with_coords": 0, "needs_geocoding": 0, "no_address": 0}
    
    def row_to_flat_doc(row):
        entry = {}
        for col in df.columns:
            try:
                val = row[col]
                if isinstance(val, pd.Series):
                    non_null_values = val.dropna()
                    if len(non_null_values) > 0:
                        entry[col] = str(non_null_values.iloc[0])
                    else:
                        entry[col] = ""
                else:
                    if pd.notna(val):
                        entry[col] = str(val)
                    else:
                        entry[col] = ""
            except Exception as e:
                entry[col] = ""

        for group, tags in TAG_GROUPS.items():
            try:
                if group == "type_tags":
                    entry[group] = collect_type_tags_from_columns(row)
                elif group == "location_name":
                    entry[group] = collect_location_name_from_columns(row)
                else:
                    collected_tags = collect_tags_from_excel_columns(row, group)
                    entry[group] = ", ".join(collected_tags)
            except Exception as e:
                entry[group] = ""

        name = clean_text(entry.get("Vrai Nom") or entry.get("Nom de base") or "")
        raw_name = clean_text(entry.get("Nom de base") or "")
        tag = clean_text(entry.get("Ref") or entry.get("tag") or "")
        address = clean_text(entry.get("Adresse") or "")
        arrondissement = arrondissement_to_code_postal(entry.get("Arrondissement"))
        phone = clean_text(entry.get("T√©l√©phone") or "")
        website = clean_text(entry.get("Site web") or "")
        reservation_link = clean_text(entry.get("Lien de r√©servation") or "")
        instagram_link = clean_text(entry.get("Lien de votre compte instagram") or "")
        instagram_video_link = clean_text(entry.get("Lien vid√©o insta") or entry.get("Lien video insta") or entry.get("Lien vid√©o instagram") or entry.get("Lien video instagram") or "")
        google_link = clean_text(entry.get("Lien Google") or "")
        lien_menu = clean_text(entry.get("Lien Menu") or "")
        hours_raw = clean_text(entry.get("Horaires") or "")
        hours_structured = process_hours(hours_raw) if hours_raw else {}
        commentaire = clean_text(entry.get("Infos") or "")
        
        latitude = None
        longitude = None
        lat_str = clean_text(entry.get("Latitude") or entry.get("latitude") or "")
        lon_str = clean_text(entry.get("Longitude") or entry.get("longitude") or "")
        
        # Log initial pour le restaurant
        restaurant_name = name or tag or "Restaurant inconnu"
        
        if lat_str:
            try:
                latitude = float(lat_str.replace(",", "."))
                if log_file:
                    log(f"üìç [{restaurant_name}] Latitude trouv√©e dans Excel: {latitude:.6f}", log_file)
            except (ValueError, AttributeError) as e:
                latitude = None
                if log_file:
                    log(f"‚ö†Ô∏è  [{restaurant_name}] Erreur parsing latitude '{lat_str}': {e}", log_file)
        if lon_str:
            try:
                longitude = float(lon_str.replace(",", "."))
                if log_file:
                    log(f"üìç [{restaurant_name}] Longitude trouv√©e dans Excel: {longitude:.6f}", log_file)
            except (ValueError, AttributeError) as e:
                longitude = None
                if log_file:
                    log(f"‚ö†Ô∏è  [{restaurant_name}] Erreur parsing longitude '{lon_str}': {e}", log_file)
        
        # V√©rifier si on a besoin de g√©ocoder
        needs_geocoding = (latitude is None or longitude is None) and address
        
        if needs_geocoding:
            geocoding_counter[0] += 1
            if log_file:
                missing = []
                if latitude is None:
                    missing.append("latitude")
                if longitude is None:
                    missing.append("longitude")
                log(f"üåç [{restaurant_name}] [{geocoding_counter[0]}/{geocoding_stats['needs_geocoding']}] D√©marrage g√©ocodage - Adresse: '{address}' - Manque: {', '.join(missing)}", log_file)
            
            coords = geocode_address(address, log_file, restaurant_name=restaurant_name)
            if coords:
                if latitude is None:
                    latitude = coords[0]
                    if log_file:
                        log(f"‚úÖ [{restaurant_name}] Latitude obtenue par g√©ocodage: {latitude:.6f}", log_file)
                if longitude is None:
                    longitude = coords[1]
                    if log_file:
                        log(f"‚úÖ [{restaurant_name}] Longitude obtenue par g√©ocodage: {longitude:.6f}", log_file)
                if log_file:
                    log(f"‚úÖ [{restaurant_name}] [{geocoding_counter[0]}/{geocoding_stats['needs_geocoding']}] G√©ocodage termin√© avec succ√®s", log_file)
                time.sleep(1.1)
            else:
                if log_file:
                    log(f"‚ùå [{restaurant_name}] [{geocoding_counter[0]}/{geocoding_stats['needs_geocoding']}] G√©ocodage √©chou√© - Aucune coordonn√©e obtenue pour: '{address}'", log_file)
        elif not address:
            if log_file:
                log(f"‚ö†Ô∏è  [{restaurant_name}] Pas d'adresse disponible pour g√©ocodage", log_file)
        else:
            if log_file:
                log(f"‚úÖ [{restaurant_name}] Coordonn√©es compl√®tes (pas de g√©ocodage n√©cessaire): lat={latitude:.6f}, lon={longitude:.6f}", log_file)
        
        ambiance_tags = collect_tags_from_excel_columns(row, "ambiance")
        price_range = collect_tags_from_excel_columns(row, "price_range")
        preferences_tags = collect_tags_from_excel_columns(row, "preferences")
        lieu_tags = collect_tags_from_excel_columns(row, "lieu_tags")
        cuisines_tags = collect_tags_from_excel_columns(row, "cuisine")
        specialite_tag = string_to_tag_list(entry.get("Sp√©cialit√©_TAG") or "")
        types_tags = collect_tags_from_excel_columns(row, "type_tags")
        moments_tags = collect_tags_from_excel_columns(row, "moment")
        terrace_tags = collect_tags_from_excel_columns(row, "terrace")
        recommended_by_tags = collect_tags_from_excel_columns(row, "recommended_by")
        
        affichage_fusionne = collect_affichage_tags(row)
        specialite_affichage = collect_specialite_affichage(row)
        
        cuisine_tag = cuisines_tags
        moment_tag = moments_tags
        lieu_tag = lieu_tags
        ambiance_tag = ambiance_tags
        preferences_tag = preferences_tags
        type_tag = types_tags
        recommended_by_tag = recommended_by_tags
        
        has_terrace = any("terrasse" in x.lower() for x in (ambiance_tags + lieu_tags + terrace_tags))
        terrace_locs = [x for x in (lieu_tags + terrace_tags) if "terrasse" in x.lower()]
        
        stations_metro = []
        station1 = entry.get("Station de metro 1", "").strip()
        lignes1 = entry.get("Lignes 1", "").strip()
        if station1 and station1.lower() not in ["non", "", "nan"]:
            lines1 = [l.strip() for l in lignes1.split(",") if l.strip()] if lignes1 else []
            stations_metro.append({"station": station1, "lines": lines1})
        
        station2 = entry.get("Stations de metro 2 ", "").strip()
        lignes2 = entry.get("Lignes 2 ", "").strip()
        if station2 and station2.lower() not in ["non", "", "nan"]:
            lines2 = [l.strip() for l in lignes2.split(",") if l.strip()] if lignes2 else []
            stations_metro.append({"station": station2, "lines": lines2})
        
        rid = normalize_id_from_tag(tag)
        doc = {
            "id": rid,
            "tag": tag,
            "name": name,
            "raw_name": raw_name,
            "address": address,
            "arrondissement": arrondissement,
            "latitude": latitude,
            "longitude": longitude,
            "phone": phone,
            "website": website,
            "reservation_link": reservation_link,
            "instagram_link": instagram_link,
            "instagram_video_link": instagram_video_link,
            "google_link": google_link,
            "lien_menu": lien_menu,
            "more_info": commentaire,
            "hours": hours_raw,
            "hours_structured": hours_structured,
            "affichage": affichage_fusionne,
            "specialite_affichage": specialite_affichage,
            "cuisine_tag": cuisine_tag,
            "specialite_tag": specialite_tag,
            "moment_tag": moment_tag,
            "lieu_tag": lieu_tag,
            "ambiance_tag": ambiance_tag,
            "preferences_tag": preferences_tag,
            "type_tag": type_tag,
            "recommended_by_tag": recommended_by_tag,
            "types": types_tags,
            "moments": moments_tags,
            "lieux": lieu_tags,
            "ambiance": ambiance_tags,
            "price_range": price_range,
            "cuisines": cuisines_tags,
            "preferences": preferences_tags,
            "recommended_by": recommended_by_tags,
            "tag_initial": [tag] if tag else [],
            "restaurant_type": types_tags,
            "location_type": lieu_tags,
            "extras": terrace_tags,
            "has_terrace": bool(has_terrace),
            "terrace_locs": terrace_locs,
            "stations_metro": stations_metro,
            "specialite_tag": specialite_tag,
            "lieu_tag": lieu_tag,
        }
        return rid, doc

    # Premi√®re passe : compter les restaurants n√©cessitant un g√©ocodage
    log("üîç Analyse pr√©liminaire : comptage des restaurants n√©cessitant un g√©ocodage...", log_file)
    geocoding_stats = {"with_coords": 0, "needs_geocoding": 0, "no_address": 0}
    
    for idx, row in rows.iterrows():
        entry = {}
        for col in df.columns:
            try:
                val = row[col]
                if isinstance(val, pd.Series):
                    non_null_values = val.dropna()
                    if len(non_null_values) > 0:
                        entry[col] = str(non_null_values.iloc[0])
                    else:
                        entry[col] = ""
                else:
                    if pd.notna(val):
                        entry[col] = str(val)
                    else:
                        entry[col] = ""
            except Exception:
                entry[col] = ""
        
        address = clean_text(entry.get("Adresse") or "")
        lat_str = clean_text(entry.get("Latitude") or entry.get("latitude") or "")
        lon_str = clean_text(entry.get("Longitude") or entry.get("longitude") or "")
        
        has_lat = bool(lat_str)
        has_lon = bool(lon_str)
        
        if has_lat and has_lon:
            geocoding_stats["with_coords"] += 1
        elif address:
            geocoding_stats["needs_geocoding"] += 1
        else:
            geocoding_stats["no_address"] += 1
    
    log(f"üìä Statistiques g√©ocodage: {geocoding_stats['with_coords']} avec coordonn√©es, {geocoding_stats['needs_geocoding']} n√©cessitent g√©ocodage, {geocoding_stats['no_address']} sans adresse", log_file)
    log(f"üåç D√©marrage du traitement avec g√©ocodage pour {geocoding_stats['needs_geocoding']} restaurants...", log_file)
    
    records = []
    ids = []
    missing_tag_rows = []
    total_rows = len(rows)
    log(f"üîÑ Conversion de {total_rows} lignes Excel en documents Firebase...", log_file)
    for count, (idx, row) in enumerate(rows.iterrows(), 1):
        if count % 10 == 0 or count == 1:
            log(f"   üìä Progression: {count}/{total_rows} restaurants trait√©s", log_file)
        rid, doc = row_to_flat_doc(row)
        if not rid:
            missing_tag_rows.append(idx + 2)
            continue
        ids.append(rid)
        records.append(doc)

    dup_counts = Counter([i for i in ids if i])
    duplicates = [k for k, c in dup_counts.items() if c > 1]

    if duplicates:
        log(f"‚ö†Ô∏è  IDs dupliqu√©s d√©tect√©s: {duplicates}", log_file)
    if missing_tag_rows:
        log(f"‚ö†Ô∏è  Lignes sans tag (Ref) : {missing_tag_rows}", log_file)

    if DEDUPE_IDS and duplicates:
        seen = {}
        for r in records:
            rid = r["id"]
            if not rid:
                continue
            if rid not in seen:
                seen[rid] = 1
            else:
                seen[rid] += 1
                r["id"] = f"{rid}-{seen[rid]}"

    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)
    with open(out_ndjson, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    csv_rows = []
    for doc in records:
        csv_rows.append({
            "id": doc.get("id", ""),
            "tag": doc.get("tag", ""),
            "name": doc.get("name", ""),
            "raw_name": doc.get("raw_name", ""),
            "address": doc.get("address", ""),
            "arrondissement": doc.get("arrondissement", ""),
            "latitude": doc.get("latitude") if doc.get("latitude") is not None else "",
            "longitude": doc.get("longitude") if doc.get("longitude") is not None else "",
            "phone": doc.get("phone", ""),
            "website": doc.get("website", ""),
            "google_link": doc.get("google_link", ""),
            "reservation_link": doc.get("reservation_link", ""),
            "instagram_link": doc.get("instagram_link", ""),
            "instagram_video_link": doc.get("instagram_video_link", ""),
            "lien_menu": doc.get("lien_menu", ""),
            "more_info": doc.get("more_info", ""),
            "hours": doc.get("hours", ""),
            "affichage": ", ".join(doc.get("affichage", [])),
            "specialite_affichage": doc.get("specialite_affichage", ""),
            "types": ", ".join(doc.get("types", [])),
            "moments": ", ".join(doc.get("moments", [])),
            "lieux": ", ".join(doc.get("lieux", [])),
            "ambiance": ", ".join(doc.get("ambiance", [])),
            "price_range": doc.get("price_range", ""),
            "cuisines": ", ".join(doc.get("cuisines", [])),
            "preferences": ", ".join(doc.get("preferences", [])),
            "recommended_by": ", ".join(doc.get("recommended_by", [])),
            "tag_initial": ", ".join(doc.get("tag_initial", [])),
            "restaurant_type": ", ".join(doc.get("restaurant_type", [])),
            "location_type": ", ".join(doc.get("location_type", [])),
            "extras": ", ".join(doc.get("extras", [])),
            "has_terrace": doc.get("has_terrace", False),
            "terrace_locs": ", ".join(doc.get("terrace_locs", [])),
            "specialite_tag": doc.get("specialite_tag", ""),
            "lieu_tag": doc.get("lieu_tag", "")
        })
    
    with open(out_csv, "w", encoding="utf-8", newline="") as cf:
        writer = csv.DictWriter(cf, fieldnames=list(csv_rows[0].keys()) if csv_rows else 
                                ["id","tag","name","raw_name","address","arrondissement","latitude","longitude","phone","website","google_link","reservation_link","instagram_link","instagram_video_link","lien_menu","more_info","hours","types","moments","lieux","ambiance","price_range","cuisines","preferences","recommended_by","has_terrace","terrace_locs","specialite_tag","lieu_tag"])
        writer.writeheader()
        for r in csv_rows:
            writer.writerow(r)
    
    log(f"üìÑ CSV temporaire cr√©√©: {out_csv}", log_file)
    return records, {"duplicates": duplicates, "missing_tag_rows": missing_tag_rows}

# -------------------- Clean & Import --------------------
def delete_collection(db, collection_name: str, batch_size: int, log_file: str) -> int:
    total_deleted = 0
    while True:
        docs = list(db.collection(collection_name).limit(batch_size).stream())
        if not docs:
            break
        batch = db.batch()
        for d in docs:
            batch.delete(d.reference)
        batch.commit()
        total_deleted += len(docs)
        log(f"üóëÔ∏è  Supprim√©s cumul√©s: {total_deleted}", log_file)
    return total_deleted

def import_records(db, collection_name: str, records: List[Dict[str, Any]], batch_size: int, log_file: str) -> int:
    imported = 0
    skipped = 0
    collection = db.collection(collection_name)
    
    for doc in records:
        rid = (doc.get("id") or "").strip()
        if not rid or not isinstance(doc, dict):
            skipped += 1
            log(f"‚è≠Ô∏è  skip: id/doc invalides ‚Üí {doc}", log_file)
            continue
        try:
            doc_ref = collection.document(rid)
            doc_ref.set(doc, merge=True)
            imported += 1
            if imported % 50 == 0:
                log(f"üì• Import√©s: {imported}", log_file)
        except Exception as e:
            log(f"‚ùå Erreur doc {rid}: {e}", log_file)
            skipped += 1
            continue
    
    log(f"üì• Import√©s: {imported}", log_file)
    return imported

def write_import_log(db, collection_logs: str, payload: Dict[str, Any], log_file: str):
    data = dict(payload)
    data["server_timestamp"] = firestore.SERVER_TIMESTAMP
    db.collection(collection_logs).add(data)
    log("üìù Log d'import √©crit dans Firestore.", log_file)

# -------------------- Favorite Count Update --------------------
def update_favorite_counts(db, log_file: str) -> Dict[str, Any]:
    """
    Met √† jour le champ favorite_count dans chaque document restaurant
    en comptant les favoris actifs depuis la collection favorites.
    
    Returns:
        Dictionnaire avec les statistiques de mise √† jour
    """
    from collections import defaultdict
    
    stats = {
        "total_favorites": 0,
        "active_favorites": 0,
        "restaurants_with_favorites": 0,
        "updated": 0,
        "errors": 0,
        "zero_initialized": 0
    }
    
    try:
        # 1) Compter les favoris actifs par restaurantId
        log("üìä R√©cup√©ration des favoris actifs...", log_file)
        favorites_ref = db.collection('favorites')
        restaurant_counts = defaultdict(int)
        
        docs = favorites_ref.stream()
        for doc in docs:
            stats["total_favorites"] += 1
            data = doc.to_dict()
            
            # V√©rifier si le favori est actif
            status = data.get('status', 'active')
            if status != 'inactive':
                stats["active_favorites"] += 1
                restaurant_id = data.get('restaurantId')
                if restaurant_id:
                    restaurant_counts[restaurant_id] += 1
        
        stats["restaurants_with_favorites"] = len(restaurant_counts)
        log(f"‚úÖ {stats['total_favorites']} favoris trouv√©s, {stats['active_favorites']} actifs, {stats['restaurants_with_favorites']} restaurants concern√©s", log_file)
        
        # 2) Mettre √† jour les compteurs dans les restaurants
        log("üîÑ Mise √† jour des compteurs dans les restaurants...", log_file)
        restaurants_ref = db.collection('restaurants')
        
        for restaurant_id, count in restaurant_counts.items():
            try:
                restaurant_ref = restaurants_ref.document(restaurant_id)
                restaurant_doc = restaurant_ref.get()
                
                if restaurant_doc.exists:
                    restaurant_ref.update({
                        'favorite_count': count
                    })
                    stats["updated"] += 1
                    if stats["updated"] % 50 == 0:
                        log(f"   ‚úì {stats['updated']} restaurants mis √† jour...", log_file)
                else:
                    log(f"   ‚ö†Ô∏è  Restaurant {restaurant_id} n'existe pas, ignor√©", log_file)
                    stats["errors"] += 1
            except Exception as e:
                log(f"   ‚ùå Erreur pour le restaurant {restaurant_id}: {e}", log_file)
                stats["errors"] += 1
        
        log(f"‚úÖ {stats['updated']} restaurants mis √† jour avec succ√®s", log_file)
        
        # 3) Initialiser √† 0 les restaurants sans favoris
        log("üîÑ Initialisation des restaurants sans favoris √† 0...", log_file)
        restaurants_with_favorites = set(restaurant_counts.keys())
        
        all_restaurants = restaurants_ref.stream()
        for restaurant_doc in all_restaurants:
            restaurant_id = restaurant_doc.id
            data = restaurant_doc.to_dict()
            
            # Si le restaurant n'a pas de favorite_count d√©fini et n'a pas de favoris
            if 'favorite_count' not in data and restaurant_id not in restaurants_with_favorites:
                try:
                    restaurant_doc.reference.update({
                        'favorite_count': 0
                    })
                    stats["zero_initialized"] += 1
                    if stats["zero_initialized"] % 50 == 0:
                        log(f"   ‚úì {stats['zero_initialized']} restaurants initialis√©s √† 0...", log_file)
                except Exception as e:
                    log(f"   ‚ùå Erreur pour le restaurant {restaurant_id}: {e}", log_file)
                    stats["errors"] += 1
        
        log(f"‚úÖ {stats['zero_initialized']} restaurants initialis√©s √† 0", log_file)
        
    except Exception as e:
        log(f"‚ùå Erreur lors de la mise √† jour des compteurs de favoris: {e}\n{traceback.format_exc()}", log_file)
        stats["errors"] += 1
        raise
    
    return stats

# -------------------- Main --------------------
def import_restaurants_from_excel(excel_path: str, sheet_name: str = "Feuil1", request=None, log_file_path=None):
    """
    Fonction principale d'import adapt√©e pour Django
    Fonctionne en DEV et en PROD - seule la base de donn√©es Firebase change selon l'environnement
    
    Args:
        excel_path: Chemin vers le fichier Excel
        sheet_name: Nom de la feuille Excel (d√©faut: "Feuil1")
        request: Objet request Django (optionnel) pour d√©terminer l'environnement Firebase
        log_file_path: Chemin du fichier de log (optionnel, sinon cr√©√© automatiquement)
    """
    # D√©tecter l'environnement Firebase (dev ou prod)
    try:
        from scripts_manager.firebase_utils import get_firebase_env_from_session
        current_env = get_firebase_env_from_session(request)
    except ImportError:
        # Fallback si firebase_utils n'est pas disponible
        current_env = os.getenv('FIREBASE_ENV', 'prod').lower()
    
    if log_file_path:
        log_file = log_file_path
        backup_dir = os.path.dirname(log_file)
    else:
        ts_dir = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        backup_dir = os.path.join(BACKUP_DIR, f"{COLLECTION_SOURCE}_{ts_dir}")
        ensure_dir(backup_dir)
        log_file = os.path.join(backup_dir, "import_run.log")
    
    log("üöÄ D√©marrage import end-to-end", log_file)
    log(f"‚Üí Excel: {excel_path}", log_file)
    log(f"‚Üí Environnement: {current_env.upper()}", log_file)

    try:
        db = init_firestore(log_file, request)
    except Exception as e:
        log(f"‚ùå Init Firestore √©chou√©e: {e}\n{traceback.format_exc()}", log_file)
        raise

    # 1) Backup
    backup_meta = {}
    try:
        log(f"üóÑÔ∏è  Backup de '{COLLECTION_SOURCE}' ...", log_file)
        backup_meta = export_collection(db, COLLECTION_SOURCE, backup_dir, log_file)
    except Exception as e:
        log(f"‚ùå Backup √©chou√©: {e}\n{traceback.format_exc()}", log_file)
        raise

    # 2) Convert Excel ‚Üí records
    try:
        out_json = os.path.join(backup_dir, "restaurants_from_excel_by_tag.json")
        out_ndjson = os.path.join(backup_dir, "restaurants_from_excel_by_tag.ndjson")
        out_csv = os.path.join(backup_dir, "restaurants_from_excel_by_tag.csv")
        log("üîÅ Conversion Excel ‚Üí JSON/NDJSON/CSV ...", log_file)
        records, conv_report = convert_excel(excel_path, sheet_name, out_json, out_ndjson, out_csv, log_file)
        if not records:
            log("‚ùå Conversion a produit 0 enregistrements. Abandon.", log_file)
            raise ValueError("Conversion a produit 0 enregistrements")
        log(f"‚úÖ Conversion OK ({len(records)} docs). Duplicates: {len(conv_report['duplicates'])}, Missing tags rows: {len(conv_report['missing_tag_rows'])}", log_file)
    except Exception as e:
        log(f"‚ùå Conversion √©chou√©e: {e}\n{traceback.format_exc()}", log_file)
        raise

    # 3) Clean collection
    try:
        log(f"üßπ Suppression de la collection '{COLLECTION_SOURCE}' ...", log_file)
        deleted = delete_collection(db, COLLECTION_SOURCE, BATCH_SIZE, log_file)
        log(f"‚úÖ Suppression termin√©e: {deleted} documents supprim√©s.", log_file)
    except Exception as e:
        log(f"‚ùå Suppression √©chou√©e: {e}\n{traceback.format_exc()}", log_file)
        raise

    # 4) Import
    try:
        log(f"üöö Import de {len(records)} documents ...", log_file)
        imported = import_records(db, COLLECTION_SOURCE, records, BATCH_SIZE, log_file)
        log(f"‚úÖ Import termin√©: {imported} documents import√©s.", log_file)
    except Exception as e:
        log(f"‚ùå Import √©chou√©: {e}\n{traceback.format_exc()}", log_file)
        raise

    # 5) Import log document
    try:
        payload = {
            "collection": COLLECTION_SOURCE,
            "imported_count": imported,
            "source": os.path.basename(out_csv),
            "timestamp": now_paris_str(),
            "backup_dir": backup_dir,
            "backup_count": backup_meta.get("count", 0),
            "backup_json_sha256": backup_meta.get("sha256_json"),
            "backup_ndjson_sha256": backup_meta.get("sha256_ndjson"),
            "duplicates_count": len(conv_report.get("duplicates", [])),
            "missing_tag_rows": conv_report.get("missing_tag_rows", []),
        }
        write_import_log(db, COLLECTION_IMPORT_LOGS, payload, log_file)
    except Exception as e:
        log(f"‚ùå √âcriture du log d'import √©chou√©e: {e}\n{traceback.format_exc()}", log_file)

    # 6) Mise √† jour des compteurs de favoris
    favorite_count_stats = {}
    try:
        log("‚ù§Ô∏è  Mise √† jour des compteurs de favoris...", log_file)
        favorite_count_stats = update_favorite_counts(db, log_file)
        log(f"‚úÖ Compteurs de favoris mis √† jour: {favorite_count_stats.get('updated', 0)} restaurants", log_file)
    except Exception as e:
        log(f"‚ö†Ô∏è  Erreur lors de la mise √† jour des compteurs de favoris: {e}\n{traceback.format_exc()}", log_file)
        # Ne pas faire √©chouer l'import si le comptage des favoris √©choue
        favorite_count_stats = {"error": str(e)}

    log("üéâ Fin de workflow end-to-end.", log_file)
    return {
        "success": True,
        "imported": imported,
        "backup_dir": backup_dir,
        "log_file": log_file,
        "duplicates": len(conv_report.get("duplicates", [])),
        "missing_tag_rows": len(conv_report.get("missing_tag_rows", [])),
        "favorite_count_stats": favorite_count_stats
    }

